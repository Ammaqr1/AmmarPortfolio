{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1257124,"sourceType":"datasetVersion","datasetId":723027}],"dockerImageVersionId":30775,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nimport pandas as pd\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-28T02:41:09.757030Z","iopub.execute_input":"2024-09-28T02:41:09.757454Z","iopub.status.idle":"2024-09-28T02:41:21.708703Z","shell.execute_reply.started":"2024-09-28T02:41:09.757414Z","shell.execute_reply":"2024-09-28T02:41:21.707651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom skimage import io\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport os\nfrom glob import glob\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom PIL import Image\n# from network import Net\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.autograd import Variable\nif not os.path.exists('./outputs'):\n    os.mkdir('./outputs')\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:41:21.710186Z","iopub.execute_input":"2024-09-28T02:41:21.710678Z","iopub.status.idle":"2024-09-28T02:41:25.394279Z","shell.execute_reply.started":"2024-09-28T02:41:21.710621Z","shell.execute_reply":"2024-09-28T02:41:25.393449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Global parameters\n\n# If USE_CUDA is True, computations will be done using the GPU (may not work in all systems)\n# This will make the calculations happen faster\nUSE_CUDA = torch.cuda.is_available()\n\nDATASET_PATH = '/kaggle/input/eurosat-rgb'\n\nBATCH_SIZE = 64 # Number of images that are used for calculating gradients at each step\n\nNUM_EPOCHS = 25 # Number of times we will go through all the training images. Do not go over 25\n\nLEARNING_RATE = 0.001 # Controls the step size\nMOMENTUM = 0.9 # Momentum for the gradient descent\nWEIGHT_DECAY = 0.0005","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:41:25.395463Z","iopub.execute_input":"2024-09-28T02:41:25.395933Z","iopub.status.idle":"2024-09-28T02:41:25.426624Z","shell.execute_reply.started":"2024-09-28T02:41:25.395897Z","shell.execute_reply":"2024-09-28T02:41:25.425584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torchvision\nfrom torchvision import datasets, transforms\n\n# Define dataset transforms\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n\n# Load the dataset\ndata_dir = DATASET_PATH  # Point to the RGB dataset\ndataset = datasets.ImageFolder(data_dir, transform=transform)\n\n# Create a DataLoader\ndata_loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n\n# Get some random images\ndata_iter = iter(data_loader)\nimages, labels = next(data_iter)  # Fetch the next batch\n\n# Function to show images\ndef imshow(img):\n    img = img / 2 + 0.5  # Unnormalize the image\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# Display images\nimshow(torchvision.utils.make_grid(images))\n\n# Print labels\nclass_names = dataset.classes\nprint('Labels:', ' '.join('%5s' % class_names[labels[j]] for j in range(4)))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:41:25.429073Z","iopub.execute_input":"2024-09-28T02:41:25.429487Z","iopub.status.idle":"2024-09-28T02:41:31.836321Z","shell.execute_reply.started":"2024-09-28T02:41:25.429450Z","shell.execute_reply":"2024-09-28T02:41:31.835230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class_names = dataset.classes\nprint(\"Classes: \", class_names)\nprint(\"Number of images: \", len(dataset))","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:42:13.647654Z","iopub.execute_input":"2024-09-28T02:42:13.648043Z","iopub.status.idle":"2024-09-28T02:42:13.654095Z","shell.execute_reply.started":"2024-09-28T02:42:13.648004Z","shell.execute_reply":"2024-09-28T02:42:13.652999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport random\n\n# Define paths\nsrc_directory = '/kaggle/input/eurosat-rgb/2750'  # Update this to your dataset path in Kaggle\ndest_directory = './split_dataset/'  # Where to save the split dataset\n\n# Create directories for train and test sets\nos.makedirs(os.path.join(dest_directory, 'train'), exist_ok=True)\nos.makedirs(os.path.join(dest_directory, 'test'), exist_ok=True)\n\n# Function to split data\ndef split_data(src_dir, dest_dir, split_ratio=0.8):\n    # Iterate through each class\n    for class_name in os.listdir(src_dir):\n        class_path = os.path.join(src_dir, class_name)\n        \n        if os.path.isdir(class_path):\n            # Create class directories in train and test\n            os.makedirs(os.path.join(dest_dir, 'train', class_name), exist_ok=True)\n            os.makedirs(os.path.join(dest_dir, 'test', class_name), exist_ok=True)\n            \n            # Get all images in the class\n            images = os.listdir(class_path)\n            random.shuffle(images)  # Shuffle images\n            \n            # Split the images\n            split_index = int(len(images) * split_ratio)\n            train_images = images[:split_index]\n            test_images = images[split_index:]\n\n            # Move images to respective directories\n            for img in train_images:\n                shutil.copy(os.path.join(class_path, img), os.path.join(dest_dir, 'train', class_name, img))\n            for img in test_images:\n                shutil.copy(os.path.join(class_path, img), os.path.join(dest_dir, 'test', class_name, img))\n\n# Execute the split\nsplit_data(src_directory, dest_directory)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:42:14.719235Z","iopub.execute_input":"2024-09-28T02:42:14.719644Z","iopub.status.idle":"2024-09-28T02:43:32.414072Z","shell.execute_reply.started":"2024-09-28T02:42:14.719604Z","shell.execute_reply":"2024-09-28T02:43:32.413237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SPLITTED_DATASET_PATH = '/kaggle/working/split_dataset'","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:43:32.416000Z","iopub.execute_input":"2024-09-28T02:43:32.416367Z","iopub.status.idle":"2024-09-28T02:43:32.420908Z","shell.execute_reply.started":"2024-09-28T02:43:32.416323Z","shell.execute_reply":"2024-09-28T02:43:32.419929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dir = os.path.join(SPLITTED_DATASET_PATH, 'train', '2750')\ntest_dir = os.path.join(SPLITTED_DATASET_PATH, 'test', '2750')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:43:32.422306Z","iopub.execute_input":"2024-09-28T02:43:32.423107Z","iopub.status.idle":"2024-09-28T02:43:32.433214Z","shell.execute_reply.started":"2024-09-28T02:43:32.423058Z","shell.execute_reply":"2024-09-28T02:43:32.432385Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if os.path.exists(train_dir):\n    shutil.rmtree(train_dir)\n    print(f\"Removed folder: {train_dir}\")\n\n# Remove the '2750' folder from the test directory if it exists\nif os.path.exists(test_dir):\n    shutil.rmtree(test_dir)\n    print(f\"Removed folder: {test_dir}\")","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:43:32.435357Z","iopub.execute_input":"2024-09-28T02:43:32.435707Z","iopub.status.idle":"2024-09-28T02:43:32.445278Z","shell.execute_reply.started":"2024-09-28T02:43:32.435671Z","shell.execute_reply":"2024-09-28T02:43:32.444452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create datasets and data loaders\n# Transformations\n\nfrom torchvision import datasets, models, transforms\ndata_transforms = transforms.Compose([\n        transforms.Resize(64),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ])\n\ntrain_dataset = datasets.ImageFolder(os.path.join(SPLITTED_DATASET_PATH, 'train'), data_transforms)\ntrain_loader = DataLoader(train_dataset, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n\n\ntest_dataset = datasets.ImageFolder(os.path.join(SPLITTED_DATASET_PATH, 'test'), data_transforms)\ntest_loader = DataLoader(test_dataset, BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n\nclass_names = train_dataset.classes\n\nprint('Dataloaders OK')\ntest_loader","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:43:32.446434Z","iopub.execute_input":"2024-09-28T02:43:32.446758Z","iopub.status.idle":"2024-09-28T02:43:32.593847Z","shell.execute_reply.started":"2024-09-28T02:43:32.446726Z","shell.execute_reply":"2024-09-28T02:43:32.592827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Create the model class\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN,self).__init__()\n        #Same Padding = [(filter size - 1) / 2] (Same Padding--> input size = output size)\n        self.cnn1 = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3,stride=1, padding=1)\n        #The output size of each of the 4 feature maps is \n        #[(input_size - filter_size + 2(padding) / stride) +1] --> [(64-3+2(1)/1)+1] = 64 (padding type is same)\n        self.batchnorm1 = nn.BatchNorm2d(4)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n  \n        #After max pooling, the output of each feature map is now 64/2 =32\n        self.cnn2 = nn.Conv2d(in_channels=4, out_channels=8, kernel_size=3, stride=1, padding=1)\n        #Output size of each of the 32 feature maps\n        self.batchnorm2 = nn.BatchNorm2d(8)\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n        \n        #After max pooling, the output of each feature map is 32/2 = 16\n        #Flatten the feature maps. You have 8 feature maps, each of them is of size 16x16 --> 8*16*16 = 2048\n        self.fc1 = nn.Linear(in_features=8*16*16, out_features=32)\n        self.droput = nn.Dropout(p=0.5)\n        self.fc2 = nn.Linear(in_features=32, out_features=10)\n        \n    def forward(self,x):\n        out = self.cnn1(x)\n        out = self.batchnorm1(out)\n        out = self.relu(out)\n        out = self.maxpool1(out)\n        out = self.cnn2(out)\n        out = self.batchnorm2(out)\n        out = self.relu(out)\n        out = self.maxpool2(out)\n        \n        #Now we have to flatten the output. This is where we apply the feed forward neural network as learned before! \n        #It will take the shape (batch_size, 2048)\n        out = out.view(x.size(0), -1)\n        \n        #Then we forward through our fully connected layer \n        out = self.fc1(out)\n        out = self.relu(out)\n        #out = self.droput(out)\n        out = self.fc2(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:04:22.625022Z","iopub.execute_input":"2024-09-28T02:04:22.625573Z","iopub.status.idle":"2024-09-28T02:04:22.636595Z","shell.execute_reply.started":"2024-09-28T02:04:22.625528Z","shell.execute_reply":"2024-09-28T02:04:22.635506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create network\nmodel = CNN()\nif USE_CUDA:\n    model = model.cuda()  \n    \nprint('Network OK')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:04:22.637998Z","iopub.execute_input":"2024-09-28T02:04:22.638500Z","iopub.status.idle":"2024-09-28T02:04:22.652899Z","shell.execute_reply.started":"2024-09-28T02:04:22.638438Z","shell.execute_reply":"2024-09-28T02:04:22.651922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define criterion, optimizer, and scheduler\n\ncriterion = nn.CrossEntropyLoss()  \noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:04:22.654432Z","iopub.execute_input":"2024-09-28T02:04:22.654868Z","iopub.status.idle":"2024-09-28T02:04:22.661960Z","shell.execute_reply.started":"2024-09-28T02:04:22.654826Z","shell.execute_reply":"2024-09-28T02:04:22.661101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Main loop\ntrain_loss = []\ntrain_accuracy = []\ntest_loss = []\ntest_accuracy = []\nepochs = []\n\nfor epoch in range(1, NUM_EPOCHS+1):\n    print(f'\\n\\nRunning epoch {epoch} of {NUM_EPOCHS}...\\n')\n    epochs.append(epoch)\n\n    #-------------------------Train-------------------------\n    \n    #Reset these below variables to 0 at the begining of every epoch\n    correct = 0\n    iterations = 0\n    iter_loss = 0.0\n    \n    model.train()  # Put the network into training mode\n    \n    for i, (inputs, labels) in enumerate(train_loader):\n       \n        if USE_CUDA:\n            inputs = inputs.cuda()\n            labels = labels.cuda()        \n            \n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        iter_loss += loss.item()  # Accumulate the loss\n        optimizer.zero_grad() # Clear off the gradient in (w = w - gradient)\n        loss.backward()   # Backpropagation \n        optimizer.step()  # Update the weights\n        \n        # Record the correct predictions for training data \n        _, predicted = torch.max(outputs, 1)\n#         correct += (predicted == labels).sum()\n        correct += (predicted == labels).sum().item() \n        iterations += 1\n        \n    scheduler.step()\n        \n    # Record the training loss\n    train_loss.append(iter_loss/iterations)\n    # Record the training accuracy\n    train_accuracy.append((100 * correct / len(train_dataset)))   \n     \n    #-------------------------Test--------------------------\n    \n    correct = 0\n    iterations = 0\n    testing_loss = 0.0\n    \n    model.eval()  # Put the network into evaluation mode\n    \n    for i, (inputs, labels) in enumerate(test_loader):\n\n        if USE_CUDA:\n            inputs = inputs.cuda()\n            labels = labels.cuda()\n        \n        outputs = model(inputs)     \n        loss = criterion(outputs, labels) # Calculate the loss\n        testing_loss += loss.item()\n        # Record the correct predictions for training data\n        _, predicted = torch.max(outputs, 1)\n#         correct += (predicted == labels).sum()\n        correct += (predicted == labels).sum().item() \n        \n        iterations += 1\n\n    # Record the Testing loss\n    test_loss.append(testing_loss/iterations)\n    # Record the Testing accuracy\n    test_accuracy.append((100 * correct / len(test_dataset)))\n   \n    print(f'\\nEpoch {epoch} validation results: Loss={test_loss[-1]} | Accuracy={test_accuracy[-1]}\\n')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:04:22.666381Z","iopub.execute_input":"2024-09-28T02:04:22.666727Z","iopub.status.idle":"2024-09-28T02:10:19.987957Z","shell.execute_reply.started":"2024-09-28T02:04:22.666688Z","shell.execute_reply":"2024-09-28T02:10:19.986778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Initialize lists to store predictions and true labels\nall_preds = []\nall_labels = []\n\n# Disable gradient calculations for validation\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        if USE_CUDA:\n            inputs, labels = inputs.cuda(), labels.cuda()\n        \n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        \n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Create confusion matrix\nconf_matrix = confusion_matrix(all_labels, all_preds)\n\n# Visualize confusion matrix using Seaborn\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:10:19.989873Z","iopub.execute_input":"2024-09-28T02:10:19.990332Z","iopub.status.idle":"2024-09-28T02:10:25.180974Z","shell.execute_reply.started":"2024-09-28T02:10:19.990279Z","shell.execute_reply":"2024-09-28T02:10:25.179901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Residential and SeaLake classes are classified very accurately, with minimal misclassification.\n \n* Highway, Pasture, and PermanentCrop show higher misclassification, particularly confusing with similar categories like HerbaceousVegetation and AnnualCrop.\n \n* River and Industrial classes have decent accuracy but still show misclassification with nearby categories like Forest and SeaLake.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Generate classification report\nreport = classification_report(all_labels, all_preds, target_names=class_names)\nprint('Classification Report:')\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:10:25.182394Z","iopub.execute_input":"2024-09-28T02:10:25.182764Z","iopub.status.idle":"2024-09-28T02:10:25.209669Z","shell.execute_reply.started":"2024-09-28T02:10:25.182725Z","shell.execute_reply":"2024-09-28T02:10:25.208768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Overall accuracy is 83%, indicating a well-performing model across the classes.\n\n* Residential, SeaLake, and Industrial classes have high precision and recall, with scores above 0.90, reflecting strong classification performance.\n\n* Highway and PermanentCrop show relatively lower performance, with precision and recall around 0.70, indicating room for improvement in distinguishing these classes.","metadata":{}},{"cell_type":"code","source":"    plt.figure(figsize=(12, 8), num=1)\n    plt.clf()\n    plt.plot(epochs, train_loss, label='Train')\n    plt.plot(epochs, test_loss, label='Test')\n    plt.legend()\n    plt.grid()\n    plt.title('Cross entropy loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.savefig('outputs/01-loss-cnn.pdf')\n    \n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:10:25.210953Z","iopub.execute_input":"2024-09-28T02:10:25.211653Z","iopub.status.idle":"2024-09-28T02:10:25.706040Z","shell.execute_reply.started":"2024-09-28T02:10:25.211580Z","shell.execute_reply":"2024-09-28T02:10:25.705158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Consistent Decrease in Training Loss:\n\n*     The training loss started at 1.46 and gradually decreased to 0.51 over 25 epochs, indicating that the model is learning and improving its performance over time.\n\n#\nStable Validation Loss:\n\n* The validation loss initially dropped from 1.19 to around 0.49 after a few epochs, showing stabilization. This suggests the model's performance is consistent and isn't overfitting.\n\n#\nNo Significant Overfitting:\n\n* Since both the training and validation losses decrease and converge around similar values, there is no evidence of overfitting, and the model generalizes well to unseen data.","metadata":{}},{"cell_type":"code","source":"    plt.figure(figsize=(12, 8), num=2)\n    plt.clf()\n    plt.plot(epochs, train_accuracy, label='Train')\n    plt.plot(epochs, test_accuracy, label='Test')\n    plt.legend()\n    plt.grid()\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.savefig('outputs/02-accuracy-cnn.pdf')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:10:25.707296Z","iopub.execute_input":"2024-09-28T02:10:25.707641Z","iopub.status.idle":"2024-09-28T02:10:26.227768Z","shell.execute_reply.started":"2024-09-28T02:10:25.707593Z","shell.execute_reply":"2024-09-28T02:10:26.226727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Training Accuracy**: Increased significantly from 47.32% to 82.39% over 25 epochs, showing effective learning.\n\n**Test Accuracy**: Reached 83.15%, indicating good generalization to unseen data and minimal overfitting.\n\n**Stability**: Training and test accuracies are converging around 83%, reflecting a well-optimized model.","metadata":{}},{"cell_type":"code","source":"data_dir = SPLITTED_DATASET_PATH \nclass_names = os.listdir(os.path.join(data_dir, 'train'))\n\nclass_counts = {class_name: len(os.listdir(os.path.join(data_dir, 'train', class_name))) for class_name in class_names}\nclass_counts_df = pd.DataFrame(list(class_counts.items()), columns=['Class', 'Count'])\n\n# Visualize Class Distribution\nplt.figure(figsize=(12, 6))\nsns.barplot(data=class_counts_df, x='Class', y='Count')\nplt.xticks(rotation=45)\nplt.title('Class Distribution in Training Dataset')\nplt.xlabel('Class')\nplt.ylabel('Number of Images')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:10:26.228889Z","iopub.execute_input":"2024-09-28T02:10:26.229173Z","iopub.status.idle":"2024-09-28T02:10:26.575256Z","shell.execute_reply.started":"2024-09-28T02:10:26.229142Z","shell.execute_reply":"2024-09-28T02:10:26.574361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Balanced Classes: Most classes have 2400 samples, ensuring good representation.**\n\n**Smaller Classes: Highway, Pasture, and others have fewer samples, with Pasture being the smallest at 1600.**","metadata":{}},{"cell_type":"code","source":"def display_sample_images(class_names, data_dir, num_samples=5):\n    plt.figure(figsize=(15, 10))\n    for i, class_name in enumerate(class_names):\n        images = os.listdir(os.path.join(data_dir, 'train', class_name))\n        selected_images = np.random.choice(images, num_samples, replace=False)\n        \n        for j, img_name in enumerate(selected_images):\n            img_path = os.path.join(data_dir, 'train', class_name, img_name)\n            img = Image.open(img_path)\n            plt.subplot(len(class_names), num_samples, i * num_samples + j + 1)\n            plt.imshow(img)\n            plt.axis('off')\n            if j == 0:\n                plt.title(class_name)\n\n    plt.tight_layout()\n    plt.show()\n\ndisplay_sample_images(class_names, data_dir)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:10:26.576762Z","iopub.execute_input":"2024-09-28T02:10:26.577192Z","iopub.status.idle":"2024-09-28T02:10:28.463151Z","shell.execute_reply.started":"2024-09-28T02:10:26.577146Z","shell.execute_reply":"2024-09-28T02:10:28.462296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_sizes = []\n\nfor class_name in class_names:\n    images = os.listdir(os.path.join(data_dir, 'train', class_name))\n    for img_name in images:\n        img_path = os.path.join(data_dir, 'train', class_name, img_name)\n        img = Image.open(img_path)\n        image_sizes.append(img.size)\n\nimage_sizes_df = pd.DataFrame(image_sizes, columns=['Width', 'Height'])\nprint(image_sizes_df.describe())\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:10:28.464577Z","iopub.execute_input":"2024-09-28T02:10:28.465093Z","iopub.status.idle":"2024-09-28T02:10:32.659583Z","shell.execute_reply.started":"2024-09-28T02:10:28.465047Z","shell.execute_reply":"2024-09-28T02:10:32.658697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**All images in the dataset have a consistent resolution of 64x64 pixels. This uniformity simplifies preprocessing since no resizing is required.**","metadata":{}},{"cell_type":"code","source":"print(f'Final train loss: {train_loss[-1]}')\nprint(f'Final test loss: {test_loss[-1]}')\nprint(f'Final train accuracy: {train_accuracy[-1]}')\nprint(f'Final test accuracy: {test_accuracy[-1]}')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:10:32.660749Z","iopub.execute_input":"2024-09-28T02:10:32.661053Z","iopub.status.idle":"2024-09-28T02:10:32.666484Z","shell.execute_reply.started":"2024-09-28T02:10:32.661020Z","shell.execute_reply":"2024-09-28T02:10:32.665635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ResNet**","metadata":{}},{"cell_type":"code","source":"#Create the model class with ResNet\ndef conv3x3(in_channels, out_channels, stride=1):\n    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n        super(ResidualBlock, self).__init__()\n        self.conv1 = conv3x3(in_channels, out_channels, stride)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(out_channels, out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n        self.downsample = downsample\n        \n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.downsample:\n            residual = self.downsample(x)\n        out += residual\n        out = self.relu(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:41:46.056382Z","iopub.execute_input":"2024-09-28T02:41:46.057297Z","iopub.status.idle":"2024-09-28T02:41:46.066907Z","shell.execute_reply.started":"2024-09-28T02:41:46.057250Z","shell.execute_reply":"2024-09-28T02:41:46.065952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_channels = 16 \n        self.conv = conv3x3(3, 16)\n        self.bn = nn.BatchNorm2d(16) \n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self.make_layer(block, 16, layers[0], 1)\n        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n        self.avg_pool = nn.AvgPool2d(16)\n        self.fc = nn.Linear(64, num_classes)\n        \n    def make_layer(self, block, out_channels, blocks, stride=1):\n        downsample = None\n        if (stride != 1) or (self.in_channels != out_channels):\n            downsample = nn.Sequential(conv3x3(self.in_channels, out_channels, stride=stride),\n                                       nn.BatchNorm2d(out_channels))\n        layers = []\n        layers.append(block(self.in_channels, out_channels, stride, downsample))\n        self.in_channels = out_channels\n        for i in range(1, blocks):\n            layers.append(block(out_channels, out_channels))\n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        out = self.conv(x)\n        out = self.bn(out)\n        out = self.relu(out)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.avg_pool(out)\n        out = out.view(x.size(0), -1)\n        out = self.fc(out)\n        return out\n    ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:41:47.303281Z","iopub.execute_input":"2024-09-28T02:41:47.303672Z","iopub.status.idle":"2024-09-28T02:41:47.316887Z","shell.execute_reply.started":"2024-09-28T02:41:47.303638Z","shell.execute_reply":"2024-09-28T02:41:47.315731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create network\nmodel = ResNet(ResidualBlock, [2, 2, 2])\nif USE_CUDA:\n    model = model.cuda()  ","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:41:48.253219Z","iopub.execute_input":"2024-09-28T02:41:48.253638Z","iopub.status.idle":"2024-09-28T02:41:48.420756Z","shell.execute_reply.started":"2024-09-28T02:41:48.253600Z","shell.execute_reply":"2024-09-28T02:41:48.419731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define criterion, optimizer, and scheduler\n\ncriterion = nn.CrossEntropyLoss()  \noptimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:41:49.601229Z","iopub.execute_input":"2024-09-28T02:41:49.601657Z","iopub.status.idle":"2024-09-28T02:41:49.607809Z","shell.execute_reply.started":"2024-09-28T02:41:49.601617Z","shell.execute_reply":"2024-09-28T02:41:49.606782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Main loop\ntrain_loss_resnet = []\ntrain_accuracy_resnet = []\ntest_loss_resnet = []\ntest_accuracy_resnet = []\nepochs = []\n\nfor epoch in range(1, NUM_EPOCHS+1):\n    print(f'\\n\\nRunning epoch {epoch} of {NUM_EPOCHS}...\\n')\n    epochs.append(epoch)\n\n    #-------------------------Train-------------------------\n    \n    #Reset these below variables to 0 at the begining of every epoch\n    correct = 0\n    iterations = 0\n    iter_loss = 0.0\n    \n    model.train()  # Put the network into training mode\n    \n    for i, (inputs, labels) in enumerate(train_loader):\n        if USE_CUDA:\n            inputs = inputs.cuda()\n            labels = labels.cuda()        \n            \n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        iter_loss += loss.item()  # Accumulate the loss\n        optimizer.zero_grad() # Clear off the gradient in (w = w - gradient)\n        loss.backward()   # Backpropagation \n        optimizer.step()  # Update the weights\n        \n        # Record the correct predictions for training data \n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n        iterations += 1\n        \n    scheduler.step()\n        \n    # Record the training loss\n    train_loss_resnet.append(iter_loss/iterations)\n    # Record the training accuracy\n    train_accuracy_resnet.append((100 * correct / len(train_dataset)))   \n     \n    #-------------------------Test--------------------------\n    \n    correct = 0\n    iterations = 0\n    testing_loss= 0.0\n    \n    model.eval()  # Put the network into evaluation mode\n    \n    for i, (inputs, labels) in enumerate(test_loader):\n\n        if USE_CUDA:\n            inputs = inputs.cuda()\n            labels = labels.cuda()\n        \n        outputs = model(inputs)     \n        loss = criterion(outputs, labels) # Calculate the loss\n        testing_loss += loss.item()\n        # Record the correct predictions for training data\n        _, predicted = torch.max(outputs, 1)\n        correct += (predicted == labels).sum().item()\n        \n        iterations += 1\n\n    # Record the Testing loss\n    test_loss_resnet.append(testing_loss/iterations)\n    # Record the Testing accuracy\n    test_accuracy_resnet.append((100 * correct / len(test_dataset)))\n   \n    print(f'\\nEpoch {epoch} validation results: Loss={test_loss_resnet[-1]} | Accuracy={test_accuracy_resnet[-1]}\\n')\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:48:35.919010Z","iopub.execute_input":"2024-09-28T02:48:35.919933Z","iopub.status.idle":"2024-09-28T02:53:30.910686Z","shell.execute_reply.started":"2024-09-28T02:48:35.919885Z","shell.execute_reply":"2024-09-28T02:53:30.909495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"    # Plot and save\n    plt.figure(figsize=(12, 8), num=1)\n    plt.clf()\n    plt.plot(epochs, train_loss_resnet, label='Train')\n    plt.plot(epochs, test_loss_resnet, label='Test')\n    plt.legend()\n    plt.grid()\n    plt.title('Cross entropy loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.savefig('outputs/01-loss-resnet.pdf')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:57:02.414958Z","iopub.execute_input":"2024-09-28T02:57:02.415439Z","iopub.status.idle":"2024-09-28T02:57:03.306033Z","shell.execute_reply.started":"2024-09-28T02:57:02.415394Z","shell.execute_reply":"2024-09-28T02:57:03.305011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Effective Learning**: ResNet's cross-entropy loss decreases from 0.554 to 0.210, indicating strong convergence and effective learning.\n\n**Architecture Advantage**: ResNet's skip connections enhance gradient flow, allowing for better feature learning compared to traditional CNNs.\n\n**Lower Final Loss**: The loss values suggest that ResNet outperforms typical CNNs, likely leading to improved accuracy in predictions.\n\n**Best Choice**: Overall, ResNet is a strong choice for complex image classification tasks due to its depth and efficiency in minimizing loss.","metadata":{}},{"cell_type":"code","source":"    plt.figure(figsize=(12, 8), num=2)\n    plt.clf()\n    plt.plot(epochs, train_accuracy_resnet, label='Train')\n    plt.plot(epochs, test_accuracy_resnet, label='Test')\n    plt.legend()\n    plt.grid()\n    plt.title('Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.savefig('outputs/02-accuracy-resnet.pdf')","metadata":{"execution":{"iopub.status.busy":"2024-09-28T02:57:08.042820Z","iopub.execute_input":"2024-09-28T02:57:08.043691Z","iopub.status.idle":"2024-09-28T02:57:08.486684Z","shell.execute_reply.started":"2024-09-28T02:57:08.043640Z","shell.execute_reply":"2024-09-28T02:57:08.485690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Overall Improvement**: The training accuracy shows a steady increase from 81.39% to 93.39%, indicating effective learning and model adaptation over epochs.\n\n**Validation Stability**: Validation accuracy fluctuates around 93%, with a peak of 94%, demonstrating that the model is robust and generalizes well on unseen data.\n\n**Convergence**: Both training and validation accuracies suggest convergence, with validation accuracy stabilizing around 93-94%, indicating no significant overfitting.\n\n**Optimal Choice**: The high accuracy levels achieved by ResNet affirm it as an excellent choice for image classification tasks, effectively capturing complex patterns in the data.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Set the model to evaluation mode\nmodel.eval()\n\n# Initialize lists to store predictions and true labels\nall_preds = []\nall_labels = []\n\n# Disable gradient calculations for validation\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        if USE_CUDA:\n            inputs, labels = inputs.cuda(), labels.cuda()\n        \n        outputs = model(inputs)\n        _, preds = torch.max(outputs, 1)\n        \n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Create confusion matrix\nconf_matrix_resnet = confusion_matrix(all_labels, all_preds)\n\n# Visualize confusion matrix using Seaborn\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix_resnet, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T03:04:34.620679Z","iopub.execute_input":"2024-09-28T03:04:34.621729Z","iopub.status.idle":"2024-09-28T03:04:37.683198Z","shell.execute_reply.started":"2024-09-28T03:04:34.621672Z","shell.execute_reply":"2024-09-28T03:04:37.682182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Strong Performance**: Classes 0, 1, and 3 show high correct predictions (556, 592, and 449), indicating effective classification.\n\n**Misclassifications**: Class 2 has notable confusion with class 6 (25 misclassifications), suggesting overlap in features.\n\n**Low Error Rates**: Overall, misclassifications are relatively low, with most classes maintaining a strong accuracy.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Generate classification report\nreport = classification_report(all_labels, all_preds, target_names=class_names)\nprint('Classification Report:')\nprint(report)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-28T03:07:15.513514Z","iopub.execute_input":"2024-09-28T03:07:15.513925Z","iopub.status.idle":"2024-09-28T03:07:15.537870Z","shell.execute_reply.started":"2024-09-28T03:07:15.513885Z","shell.execute_reply":"2024-09-28T03:07:15.536823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**High Overall Performance**: The model achieves an accuracy of 94%, demonstrating effective classification across all classes.\n\n**Consistent Precision and Recall**: Most classes, particularly Forest (0.99) and Residential (0.98), show high precision and recall, indicating reliable predictions and minimal false positives.\n\n**Areas for Improvement**: Classes like PermanentCrop (0.89 precision, 0.87 recall) and River (0.93 precision, 0.87 recall) show slightly lower scores, highlighting potential areas for further refinement to boost overall model performance.","metadata":{}},{"cell_type":"code","source":"_____","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"___________________________________________________________________________________________\n\n\n# Conclusion\n**Overall, the ResNet model outperforms the CNN in terms of classification accuracy and reliability, making it a preferable choice for satellite image classification tasks. However, both models have shown good results, and future enhancements could focus on improving the performance in the classes where they struggle the most.**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}